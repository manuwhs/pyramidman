{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio Basics\n",
    "\n",
    "A walkthough the different functions in pyramidman that handle the recording, playing and processing of audio. It is mainly for didactic and testing purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pyramidman.audio_parameters import AudioParameters\n",
    "from pyramidman.basic_audio_IO import play_audio, record_audio\n",
    "from pyramidman.audio_utils import get_available_microphones, get_sysdefault_microphone_index, get_all_devices_str\n",
    "from pyramidman.queue_utils import record_with_queue\n",
    "from pyramidman.unwrapper import unwrap\n",
    "\n",
    "from pyramidman.Ihy import get_audio_menu_wav_file\n",
    "\n",
    "from pyramidman.audio_utils import calibrate_microphone\n",
    "\n",
    "\n",
    "import speech_recognition as sr\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "from scipy.io import wavfile\n",
    "\n",
    "import plotly.graph_objs as go\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "supurious knowledge. So apparently speech_recoginition uses pyaudio which uses pyport to read from the strem of device. When the streem is open it will dump the recordings in a buffer from which we can read. When we read, we specify tnumber of samples (will be intenally multiplied by the size of the bytes of the sample). I do not know how big is the buffer but I know that when we read, it starts from the beginning. It is a blocking function until we have read as many samples as we specified. \n",
    "\n",
    "There is the following problems:\n",
    "- The function to know how many samples there are to be read does not work, it just outputs the chunck size. But the data gets accumulated in the buffer from the moment we opent the stream if we do not read it. \n",
    "- Sometimes the initial 2 seconds of the stream is bullshit, it starts at the minimum for 16 bits, -32000 and the grows to 0 (its mean) over the span of 2 seconds. \n",
    "\n",
    "I do not know why this happens but a possible solution would be to just open the stream, sleep 2 seconds, empty the stream by reading enough samples (can be checked if the number of bytes left to read is lower than a chunck, then we know we are good. Then start as normal.\n",
    "\n",
    "Also I do not know how to regulate the gain of the bloody microphone so that it does not overflow. It seems unlikely to be done in python, it might require specify OS calls for each case.\n",
    "\n",
    "Also I need a way to filter the high energy noise that is not voice, maybe by fourier transform if the intensity of the requency of the "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instance of the class AudioParameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<AudioParameters>\tobject has children:\n",
      "    <int>\tchunk:\t1024\n",
      "    <int>\tsample_format:\t8\n",
      "    <NoneType>\tsubtype:\tNone\n",
      "    <int>\tchannels:\t1\n",
      "    <int>\tsample_rate:\t48000\n",
      "    <int>\tinput_device_index:\t0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "audio_params = AudioParameters()\n",
    "unwrap(audio_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'HDA Intel PCH: ALC295 Analog (hw:0,0)',\n",
       " 'hostapi': 0,\n",
       " 'max_input_channels': 2,\n",
       " 'max_output_channels': 2,\n",
       " 'default_low_input_latency': 0.005804988662131519,\n",
       " 'default_low_output_latency': 0.005804988662131519,\n",
       " 'default_high_input_latency': 0.034829931972789115,\n",
       " 'default_high_output_latency': 0.034829931972789115,\n",
       " 'default_samplerate': 44100.0}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_params.get_input_device_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information of avilable devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   0 HDA Intel PCH: ALC295 Analog (hw:0,0), ALSA (2 in, 2 out)\n",
       "   1 HDA Intel PCH: HDMI 0 (hw:0,3), ALSA (0 in, 8 out)\n",
       "   2 HDA Intel PCH: HDMI 1 (hw:0,7), ALSA (0 in, 8 out)\n",
       "   3 HDA Intel PCH: HDMI 2 (hw:0,8), ALSA (0 in, 8 out)\n",
       "   4 HDA Intel PCH: HDMI 3 (hw:0,9), ALSA (0 in, 8 out)\n",
       "   5 HDA Intel PCH: HDMI 4 (hw:0,10), ALSA (0 in, 8 out)\n",
       "   6 sysdefault, ALSA (128 in, 128 out)\n",
       "   7 front, ALSA (0 in, 2 out)\n",
       "   8 surround40, ALSA (0 in, 2 out)\n",
       "   9 surround51, ALSA (0 in, 2 out)\n",
       "  10 surround71, ALSA (0 in, 2 out)\n",
       "  11 hdmi, ALSA (0 in, 8 out)\n",
       "  12 pulse, ALSA (32 in, 32 out)\n",
       "  13 dmix, ALSA (0 in, 2 out)\n",
       "* 14 default, ALSA (32 in, 32 out)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_all_devices_str()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': 'HDA Intel PCH: ALC295 Analog (hw:0,0)',\n",
       " '6': 'sysdefault',\n",
       " '12': 'pulse',\n",
       " '14': 'default'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "devices_dict = get_available_microphones()\n",
    "devices_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play audio\n",
    "\n",
    "We pass it a AudioParameters instance, from it, it is going to get chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_to_play = \"../audios/standard/english.wav\"\n",
    "play_audio(audio_params, filename = file_to_play )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recording audio\n",
    "\n",
    "First we should select a proper mycrophone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<AudioParameters>\tobject has children:\n",
      "    <int>\tchunk:\t1024\n",
      "    <int>\tsample_format:\t8\n",
      "    <NoneType>\tsubtype:\tNone\n",
      "    <int>\tchannels:\t1\n",
      "    <int>\tsample_rate:\t48000\n",
      "    <int>\tinput_device_index:\t6\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "audio_params.set_sysdefault_microphone_index()\n",
    "audio_params.set_default_input_parameters()\n",
    "unwrap(audio_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call the recording function with the correct parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording\n",
      "Finished recording\n"
     ]
    }
   ],
   "source": [
    "file_to_record = \"../audios/temp/recording.wav\"\n",
    "record_audio(audio_params, seconds = 3, filename = file_to_record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play the recorded audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_audio(audio_params,  filename = file_to_record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Record using queues\n",
    "\n",
    "It seems better, with way less in between cuts that are probably because there is less delay due to processing of the chucks. But still as time grows, this shit takes too much time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Recording finished: \n"
     ]
    }
   ],
   "source": [
    "filename_w = \"../audios/temp/caec2.wav\"\n",
    "record_with_queue(audio_params, filename_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_audio(audio_params, filename_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a Microphone\n",
    "\n",
    "The speech_recognition library has a Microphone class that is helpful at recording data. We can get one instance directly from the AudioParameters class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "mic = audio_params.get_microphone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mic.device_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['HDA Intel PCH: ALC295 Analog (hw:0,0)',\n",
       " 'HDA Intel PCH: HDMI 0 (hw:0,3)',\n",
       " 'HDA Intel PCH: HDMI 1 (hw:0,7)',\n",
       " 'HDA Intel PCH: HDMI 2 (hw:0,8)',\n",
       " 'HDA Intel PCH: HDMI 3 (hw:0,9)',\n",
       " 'HDA Intel PCH: HDMI 4 (hw:0,10)',\n",
       " 'sysdefault',\n",
       " 'front',\n",
       " 'surround40',\n",
       " 'surround51',\n",
       " 'surround71',\n",
       " 'hdmi',\n",
       " 'pulse',\n",
       " 'dmix',\n",
       " 'default']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mic.list_microphone_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this instance to together with the recognizer of the library in order to capture text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<speech_recognition.AudioData at 0x7f13a9416a50>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = sr.Recognizer()\n",
    "with mic as source:                # use the default microphone as the audio source\n",
    "    audio = r.record(source, duration = 3)                   # listen for the first phrase and extract it into audio data\n",
    "\n",
    "audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<AudioData>\tobject has children:\n",
      "    <bytes>\tframe_data\n",
      "    <int>\tsample_rate:\t48000\n",
      "    <int>\tsample_width:\t2\n",
      "\n",
      "  <bytes>\tframe_data has children:\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unwrap(audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_mic = '../audios/temp/hello_world.wav'\n",
    "\n",
    "with mic as source:\n",
    "    audio = r.record(source,duration = 5)\n",
    "\n",
    "with open(filename_mic, \"wb\") as f:\n",
    "    f.write(audio.get_wav_data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_audio(audio_params, filename_mic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAMPLE_WIDTH:  2\n"
     ]
    }
   ],
   "source": [
    "    # Number of bytes in the\n",
    "print(\"SAMPLE_WIDTH: \", mic.SAMPLE_WIDTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert audio to numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bytes"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_raw_data = audio.frame_data\n",
    "audio_raw_data = audio.get_raw_data()\n",
    "type(audio_raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1502, -1436, -1600, ..., -4596, -4376, -4170], dtype=int16)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_array = np.frombuffer(audio.frame_data, np.int16)\n",
    "audio_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting of the audiowave."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7b5b47e5ba8418286b998b681f3f9b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tab(children=(FigureWidget({\n",
       "    'data': [{'line': {'color': 'deepskyblue'},\n",
       "              'name': 'AAPL High'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tabs = get_audio_menu_wav_file(filename_mic)\n",
    "display(tabs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Own listener implementation\n",
    "\n",
    "Since the listener from the speech_recognition library was not good enough, we have implemented another one.\n",
    "\n",
    "The main changes are:\n",
    "- Refactoring of code.\n",
    "- Removing the snowboy option\n",
    "- Adding timestamp of the returned data.\n",
    "- Adding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyramidman.listener import listen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "non_speaking_duration:  0.5\n",
      "pause_threshold:  0.8\n",
      "phrase_threshold:  0.3\n",
      "energy_threshold:  1614.8541607580403\n"
     ]
    }
   ],
   "source": [
    "with mic as source:\n",
    "    r.adjust_for_ambient_noise(mic, duration = 1)\n",
    "    \n",
    "def print_recognizer_parameters(r):\n",
    "    # Maximum number of seconds of non-speaking seconds before and after the audio\n",
    "    print(\"non_speaking_duration: \", r.non_speaking_duration)\n",
    "\n",
    "    # Number of non-speaking seconds to be considered end of sentence.\n",
    "    print(\"pause_threshold: \", r.pause_threshold)\n",
    "\n",
    "    # Minimum number of seconds of a sentence.\n",
    "    print(\"phrase_threshold: \", r.phrase_threshold)\n",
    "\n",
    "    # The amount of energy in \n",
    "    print(\"energy_threshold: \", r.energy_threshold)\n",
    "\n",
    "print_recognizer_parameters(r)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples to read: 4800 Bytes read:  9600 Available bits:  2048\n",
      "Samples to read: 4800 Bytes read:  9600 Available bits:  2048\n",
      "Samples to read: 4800 Bytes read:  9600 Available bits:  2048\n",
      "Samples to read: 4800 Bytes read:  9600 Available bits:  2048\n",
      "Samples to read: 4800 Bytes read:  9600 Available bits:  2048\n",
      "Samples to read: 4800 Bytes read:  9600 Available bits:  2048\n",
      "Samples to read: 4800 Bytes read:  9600 Available bits:  2048\n",
      "Samples to read: 4800 Bytes read:  9600 Available bits:  2048\n",
      "Samples to read: 4800 Bytes read:  9600 Available bits:  2048\n",
      "Samples to read: 4800 Bytes read:  9600 Available bits:  2048\n",
      "Samples to read: 4800 Bytes read:  9600 Available bits:  56\n"
     ]
    },
    {
     "ename": "WaitTimeoutError",
     "evalue": "listening timed out while waiting for phrase to start",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mWaitTimeoutError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-177-a371df15ebb0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mmic\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0maudio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlisten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphrase_time_limit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/VScode/pyramidman/pyramidman/listener.py\u001b[0m in \u001b[0;36mlisten\u001b[0;34m(r, source, timeout, phrase_time_limit)\u001b[0m\n\u001b[1;32m    123\u001b[0m             \u001b[0mframes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m             \u001b[0mis_timeout_expired\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melapsed_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0;31m# ensure we only keep the needed amount of non-speaking buffers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/VScode/pyramidman/pyramidman/listener.py\u001b[0m in \u001b[0;36mis_timeout_expired\u001b[0;34m(timeout, elapsed_time)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0melapsed_time\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         raise WaitTimeoutError(\n\u001b[0;32m---> 12\u001b[0;31m             \"listening timed out while waiting for phrase to start\")\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mWaitTimeoutError\u001b[0m: listening timed out while waiting for phrase to start"
     ]
    }
   ],
   "source": [
    "import time\n",
    "r.non_speaking_duration = 0.2\n",
    "r.pause_threshold = 1.2\n",
    "r.energy_threshold = 3000\n",
    "with mic as source:\n",
    "    time.sleep(1)\n",
    "    audio = listen(r, source, timeout = 1, phrase_time_limit=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(filename_mic, \"wb\") as f:\n",
    "    f.write(audio.get_wav_data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaee71528c3b4025aff941a72511effb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tab(children=(FigureWidget({\n",
       "    'data': [{'line': {'color': 'deepskyblue'},\n",
       "              'name': 'AAPL High'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tabs = get_audio_menu_wav_file(filename_mic)\n",
    "display(tabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
