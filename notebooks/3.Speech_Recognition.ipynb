{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech Recognition techniques\n",
    "\n",
    "A walkthough the different functions in pyramidman that handle the recording, playing and processing of audio. It is mainly for didactic and testing purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pyramidman.audio_parameters import AudioParameters\n",
    "from pyramidman.basic_audio_IO import play_audio, record_audio\n",
    "from pyramidman.audio_utils import get_available_microphones, get_sysdefault_microphone_index, get_all_devices_str\n",
    "from pyramidman.queue_utils import record_with_queue\n",
    "from pyramidman.unwrapper import unwrap\n",
    "from pyramidman.speech_recognizing import recognize_speech_from_mic\n",
    "\n",
    "from pyramidman.hieroglyph import plot_timeseries_range_slider, create_tabs, plot_spectrogram\n",
    "from pyramidman.hieroglyph import add_word_annotations\n",
    "from pyramidman.Ihy import get_audio_menu_wav_file\n",
    "from pyramidman.audio_utils import calibrate_microphone\n",
    "\n",
    "from pyramidman.deepspeech_tools import transcribe, DeepSpeechArgs\n",
    "\n",
    "import speech_recognition as sr\n",
    "import time\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "from scipy.io import wavfile\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib qt\n",
    "\n",
    "import plotly.graph_objs as go\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "\n",
    "from queue import Queue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instantiate and calibrate microphone\n",
    "\n",
    "Ideally, we would like a background process in a thread that whenever a sentence is finished, it is translated and plotted. This is the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_params = AudioParameters()\n",
    "audio_params.set_sysdefault_microphone_index()\n",
    "audio_params.set_default_input_parameters()\n",
    "# audio_params.sample_rate = 16000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calibrating microphone for 3 seconds.\n",
      "Calibrated energy threshold:  2279.443015006792\n"
     ]
    }
   ],
   "source": [
    "mic = audio_params.get_microphone()\n",
    "r = sr.Recognizer()\n",
    "calibrate_microphone(mic, r, duration = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Record example message to process with the recognizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_mic = '../audios/temp/hello_world.wav'\n",
    "\n",
    "with mic as source:\n",
    "    audio = r.listen(source)\n",
    "\n",
    "with open(filename_mic, \"wb\") as f:\n",
    "    f.write(audio.get_wav_data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "297dafbc0b1f45d59df24690fce1f2eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tab(children=(FigureWidget({\n",
       "    'data': [{'line': {'color': 'deepskyblue'},\n",
       "              'name': 'AAPL High'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tabs = get_audio_menu_wav_file(filename_mic)\n",
    "display(tabs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech recognition\n",
    "\n",
    "In the following we will talk about the different transcribers implemented and how to use them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. speech_recognition library\n",
    "\n",
    "The speech recognition library has a set of ready to use transcribers:\n",
    "- sphinx: More software needed to be installed locally.\n",
    "- google: Limited number of calls.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = recognize_speech_from_mic(audio_params, duration = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Success : True\n",
      "Error   : Unable to recognize speech\n",
      "\n",
      "Text from Speech\n",
      "-----------------\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print('\\nSuccess : {}\\nError   : {}\\n\\nText from Speech\\n{}\\n\\n{}' \\\n",
    "      .format(response['success'],\n",
    "              response['error'],\n",
    "              '-'*17,\n",
    "              response['transcription']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. DeepSpeech recognizer\n",
    "\n",
    "We have installed and used this decoder and created some code on top to easily use it. \n",
    "\n",
    "It has the following peculiarities:\n",
    "- It only works for 16000 sample_rate data, so we need to resample the 48000 recording. Still it is better to record in 48000 and then filter (no just downsamplling) due to aliasing\n",
    "- It can return metadata with the likelihood of the transformation and the start_time and duration of each word.\n",
    "- There seems to be an error in the decoder as it always assings the time 0 to the first letter it decodes, independently of when it happens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<DeepSpeechArgs>\tobject has children:\n",
      "    <str>\tmodel:\t../models/deepspeech/deepspeech-0.6.0-mo\n",
      "    <str>\tlm:\t../models/deepspeech/deepspeech-0.6.0-mo\n",
      "    <str>\ttrie:\t../models/deepspeech/deepspeech-0.6.0-mo\n",
      "    <int>\tbeam_width:\t500\n",
      "    <float>\tlm_alpha:\t0.75\n",
      "    <float>\tlm_beta:\t1.85\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "args = DeepSpeechArgs()\n",
    "unwrap(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = transcribe(args, filename_mic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata[\"sentence\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'word': 'i', 'start_time ': 0.0, 'duration': 0.0}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata[\"words\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['i', 0.0, 0]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[item.character,item.start_time, item.timestep] for item in metadata[\"characters\"].items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a26c105a938c40b98ef007b09a3d44b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tab(children=(FigureWidget({\n",
       "    'data': [{'line': {'color': 'deepskyblue'},\n",
       "              'name': 'AAPL High'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tabs = get_audio_menu_wav_file(filename_mic)\n",
    "add_word_annotations(tabs.children[0],metadata[\"words\"])\n",
    "display(tabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_audio(audio_params, filename_mic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic tuning for transcription.\n",
    "\n",
    "We could store my voice for a few sentences and write down what it should be, then play with preprocessing parameters and the transcriber to find the best set up!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transcriber class\n",
    "Its main goal is to manage the recording and transcription of the data. It makes it available through a time queue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyramidman.Seshat import Transcriber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Transcriber>\tobject has children:\n",
      "    <bool>\t_listening:\tFalse\n",
      "    <bool>\t_transcribing:\tFalse\n",
      "    <Queue>\t_audios_queue\n",
      "    <Queue>\t_transcriptions_queue\n",
      "    <NoneType>\t_stop_listen_in_background_func:\tNone\n",
      "    <NoneType>\t_stop_transcribing_in_background_func:\tNone\n",
      "    <int>\titem_index:\t0\n",
      "    <list>\t_transcriptions\n",
      "\n",
      "  <list>\t_transcriptions has children:\n",
      "\n",
      "  <Queue>\t_transcriptions_queue has children:\n",
      "\n",
      "  <Queue>\t_audios_queue has children:\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transcriber = Transcriber()\n",
    "unwrap(transcriber, max_level = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiating and calibrating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calibrating microphone for 1 seconds.\n",
      "Calibrated energy threshold:  3641.8891025080434\n"
     ]
    }
   ],
   "source": [
    "# Recordings folder is where the sentences will be stored\n",
    "transcriber.set_automatic_default_recording_variables(recordings_folder=\"../audios/temp/\")\n",
    "transcriber.set_automatic_default_transcribing_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Transcriber>\tobject has children:\n",
      "    <bool>\t_listening:\tFalse\n",
      "    <bool>\t_transcribing:\tFalse\n",
      "    <Queue>\t_audios_queue\n",
      "    <Queue>\t_transcriptions_queue\n",
      "    <NoneType>\t_stop_listen_in_background_func:\tNone\n",
      "    <NoneType>\t_stop_transcribing_in_background_func:\tNone\n",
      "    <int>\titem_index:\t0\n",
      "    <list>\t_transcriptions\n",
      "    <str>\trecordings_folder:\t../audios/temp/\n",
      "    <AudioParameters>\taudio_params\n",
      "    <Microphone>\tmicrophone\n",
      "    <Recognizer>\trecognizer\n",
      "    <Recognizer>\trecognizer\n",
      "    <Recognizer>\trecognizer\n",
      "\n",
      "  <Recognizer>\trecognizer has children:\n",
      "\n",
      "  <Recognizer>\trecognizer has children:\n",
      "\n",
      "  <Recognizer>\trecognizer has children:\n",
      "\n",
      "  <Microphone>\tmicrophone has children:\n",
      "\n",
      "  <AudioParameters>\taudio_params has children:\n",
      "\n",
      "  <list>\t_transcriptions has children:\n",
      "\n",
      "  <Queue>\t_transcriptions_queue has children:\n",
      "\n",
      "  <Queue>\t_audios_queue has children:\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unwrap(transcriber, max_level = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listening \n",
    "\n",
    "When the we start listening, we reinitialize the queue.\n",
    "When we stop listening, it still exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcriber.start_listening_in_background(phrase_time_limit=5, timeout=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcriber.is_listening()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcriber.stop_listening_in_background()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcriber.is_listening()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "audios_queue = transcriber.get_audios_queue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data was stored\n"
     ]
    }
   ],
   "source": [
    "if audios_queue.empty() == False:\n",
    "    data = audios_queue.get()\n",
    "    print(data)\n",
    "else:\n",
    "    print(\"No data was stored\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transcribing \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcriber.start_transcribing_in_background()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcriber.is_transcribing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcriber.stop_transcribing_in_background()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcriptions_queue = transcriber.get_transcriptions_queue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentence': 'i', 'words': [{'word': 'i', 'start_time ': 0.0, 'duration': 0.0}], 'characters': <Swig Object of type 'Metadata *' at 0x7f02fe838630>, 'confidence': -9.85685304694977}\n"
     ]
    }
   ],
   "source": [
    "if transcriptions_queue.empty() == False:\n",
    "    data = transcriptions_queue.get()\n",
    "    print(data)\n",
    "else:\n",
    "    print(\"No dat was stored\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
