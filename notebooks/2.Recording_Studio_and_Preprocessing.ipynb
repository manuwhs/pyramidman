{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recording Studio\n",
    "\n",
    "This notebook is intended to record, preprocess and save the audios that will be later used by pyramidman assistant. It will make use of the speech recognizing as well for practical purposes, but theses will not be explained in this notebook, but rather in the third one.\n",
    "\n",
    "This notebook focuses on making a proper listener in another thread that writes the audio data into a Queue that is later consumed by the main thread and in having a simple Recording Studio for making the audios for pyramidman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/montoya/anaconda3/envs/python36/lib/python3.7/site-packages/tqdm/autonotebook.py:17: TqdmExperimentalWarning:\n",
      "\n",
      "Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "from pyramidman.audio_parameters import AudioParameters\n",
    "from pyramidman.basic_audio_IO import play_audio, record_audio\n",
    "from pyramidman.audio_utils import get_available_microphones, get_sysdefault_microphone_index, get_all_devices_str\n",
    "from pyramidman.queue_utils import record_with_queue\n",
    "from pyramidman.unwrapper import unwrap\n",
    "from pyramidman.speech_recognizing import recognize_speech_from_mic\n",
    "from pyramidman.hieroglyph import plot_timeseries_range_slider, create_tabs, plot_spectrogram\n",
    "from pyramidman.hieroglyph import add_word_annotations\n",
    "\n",
    "from pyramidman.Ihy import get_audio_menu_wav_file\n",
    "from pyramidman.signal_processing import get_spectrogram\n",
    "\n",
    "from pyramidman.queue_utils import put_data_in_queue_closure, listen_in_a_thread\n",
    "from pyramidman.audio_utils import calibrate_microphone, sample_noise\n",
    "\n",
    "from pyramidman.utils import get_folder_files\n",
    "import speech_recognition as sr\n",
    "from pyramidman.deepspeech_tools import transcribe, DeepSpeechArgs\n",
    "\n",
    "import plotly\n",
    "import time\n",
    "\n",
    "%matplotlib qt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import plotly.graph_objs as go\n",
    "from IPython.display import display\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "from scipy.io import wavfile\n",
    "import ipywidgets as widgets\n",
    "\n",
    "from queue import Queue\n",
    "import noisereduce as nr\n",
    "import librosa\n",
    "\n",
    "from pyramidman.noisereduce_optimized import reduce_noise_optimized\n",
    "from pyramidman.noisereduce_optimized import noise_STFT_and_statistics\n",
    "from pyramidman.noisereduce_optimized import reduce_noise_optimized_closure\n",
    "from pyramidman.signal_processing import butter_highpass_filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Instantiate and calibrate microphone\n",
    "\n",
    "Ideally, we would like a background process in a thread that whenever a sentence is finished, it is translated and plotted. This is the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_params = AudioParameters()\n",
    "audio_params.set_sysdefault_microphone_index()\n",
    "audio_params.set_default_input_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_params.input_device_index = 6\n",
    "audio_params.sample_rate = 48000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calibrating microphone for 1 seconds.\n",
      "Calibrated energy threshold:  1085.7585450362508\n"
     ]
    }
   ],
   "source": [
    "mic = audio_params.get_microphone()\n",
    "r = sr.Recognizer()\n",
    "\n",
    "calibrate_microphone(mic, r, duration = 1, warmup_duration = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74.25"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "178.200*(5/6)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "178.200/44.55"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Reduce noise and trim signal\n",
    "\n",
    "In this section we show some of the functions used in order to reduce the noise of a recorded signal. The goal is mainly didactic, later in the Recording studio, we can play more with these functions to find the optimal preprocessing of the signal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Record sample audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording\n",
      "Finished recording\n"
     ]
    }
   ],
   "source": [
    "recorded_filepath = \"../audios/temp/recording.wav\"\n",
    "record_audio(audio_params, seconds = 5, filename = recorded_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_audio(audio_params, recorded_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Reduce noise\n",
    "\n",
    "We can apply several filters and advanced techniques. In this case we mainly used an advanced library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_data = sample_noise(audio_params,r, mic, duration = 2, warmup = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the audio data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rate, data = wavfile.read(recorded_filepath)\n",
    "data = data.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform the noise reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_noise = reduce_noise_optimized(audio_clip=data, noise_clip=noise_data)\n",
    "reduced_noise = reduced_noise.astype(np.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "recorded_filepath_noise_reduced = \"../audios/temp/recording_reduced.wav\"\n",
    "wavfile.write(recorded_filepath_noise_reduced, audio_params.sample_rate, reduced_noise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Closurized version\n",
    "\n",
    "So that it is callable with one the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\xad\\xf3t\\xfdJ>\\xb21\\xe5\\xf5\\xb8\\x02\\x83\\xf5\\xcf3\\xe5\\xf6\\xea\\x04\\xde\\xbd_5\\x17\\xf4a\\xfb=F'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noise_params_list = noise_STFT_and_statistics(noise_data)\n",
    "noise_stft, noise_stft_db, mean_freq_noise, std_freq_noise, noise_thresh = noise_params_list\n",
    "\n",
    "reduce_noise_optimized_closurized = reduce_noise_optimized_closure(noise_data)\n",
    "\n",
    "reduce_noise_optimized_closurized(data)[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Trim signal\n",
    "\n",
    "Mainly at the beggining and end of the signal in order to reduce the length of the signal given later to the transcriber. It is mainly based on detecting when the power of the signal is below a given threshold. More advanced approaches based on the power at given frequencies could be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_noise_reduced_and_trimmed, index = librosa.effects.trim(reduced_noise.astype(float),top_db=20, ref=np.max, frame_length=512*4, hop_length=256*4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "recorded_filepath_noise_reduced_and_trimmed = \"../audios/temp/recording_reduced_and_trimmed.wav\"\n",
    "wavfile.write(recorded_filepath_noise_reduced_and_trimmed, audio_params.sample_rate, audio_noise_reduced_and_trimmed.astype(np.int16))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We also just trim the original signal for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_trimmed, index = librosa.effects.trim(data.astype(float),top_db=20, ref=np.max, frame_length=512*4, hop_length=256*4)\n",
    "\n",
    "recorded_filepath_trimmed = \"../audios/temp/recording_trimmed.wav\"\n",
    "wavfile.write(recorded_filepath_trimmed, audio_params.sample_rate, audio_trimmed.astype(np.int16))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e60de23b940440c80ea5870cd8e8d36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tab(children=(Tab(children=(FigureWidget({\n",
       "    'data': [{'line': {'color': 'deepskyblue'},\n",
       "              'name…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tabs_1 = get_audio_menu_wav_file(recorded_filepath)\n",
    "tabs_2 = get_audio_menu_wav_file(recorded_filepath_noise_reduced)\n",
    "tabs_3 = get_audio_menu_wav_file(recorded_filepath_trimmed)\n",
    "tabs_4 = get_audio_menu_wav_file(recorded_filepath_noise_reduced_and_trimmed)\n",
    "\n",
    "tabs = create_tabs([tabs_1, tabs_2, tabs_3, tabs_4], [\"Original\", \"Filtered\",\"Trimmed\", \"Filtered and Trimmered\"])\n",
    "display(tabs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Transcribe to know which approach is better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = DeepSpeechArgs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'get gay oostenaula less'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "play_audio(audio_params, recorded_filepath)\n",
    "transcribe(args, recorded_filepath)[\"sentence\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'he is loose he lie'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "play_audio(audio_params, recorded_filepath_noise_reduced)\n",
    "transcribe(args, recorded_filepath_noise_reduced)[\"sentence\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'get gay oostenaula less'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "play_audio(audio_params, recorded_filepath_trimmed)\n",
    "transcribe(args, recorded_filepath_trimmed)[\"sentence\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'o most of them the lie'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "play_audio(audio_params, recorded_filepath_noise_reduced_and_trimmed)\n",
    "transcribe(args, recorded_filepath_noise_reduced_and_trimmed)[\"sentence\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Low pass filter to remove the low frequency components noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording\n",
      "Finished recording\n"
     ]
    }
   ],
   "source": [
    "recorded_filepath = \"../audios/temp/recording_filter_noise.wav\"\n",
    "record_audio(audio_params, seconds = 3, filename = recorded_filepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "rate, data = wavfile.read(recorded_filepath)\n",
    "data = data.astype(float)\n",
    "reduced_noise = butter_highpass_filter(data, cutoff = 100, fs = 48000, order=5)\n",
    "reduced_noise = reduced_noise.astype(np.int16)\n",
    "\n",
    "recorded_filepath_noise_reduced = \"../audios/temp/recording_filter_noise_reduced.wav\"\n",
    "wavfile.write(recorded_filepath_noise_reduced, audio_params.sample_rate, reduced_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d24586b2483244c69e6cd08f81d4d64d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tab(children=(Tab(children=(FigureWidget({\n",
       "    'data': [{'line': {'color': 'deepskyblue'},\n",
       "              'name…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tabs_1 = get_audio_menu_wav_file(recorded_filepath)\n",
    "tabs_2 = get_audio_menu_wav_file(recorded_filepath_noise_reduced)\n",
    "\n",
    "tabs = create_tabs([tabs_1, tabs_2], [\"Original\", \"Filtered\"])\n",
    "display(tabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "play_audio(audio_params, recorded_filepath)\n",
    "play_audio(audio_params, recorded_filepath_noise_reduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Recording studio.\n",
    "\n",
    "We have created a simple plotly UI to record and save the audios for the pyramidman assistant. This can be reused in the future for extension of capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3366509392434664a8647e45eaad4b74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(VBox(children=(Output(), Text(value='../audios/temp/', description='Folder:', la…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Global variables\n",
    "mic = audio_params.get_microphone()\n",
    "r = sr.Recognizer()\n",
    "\n",
    "# Box with the recordings to show.\n",
    "figure_box = widgets.Box([go.FigureWidget()], layout = {\"width\":\"70%\", \"height\":\"600px\"})\n",
    "\n",
    "######### Panel widgets \n",
    "\n",
    "## Recording menu\n",
    "recording_title_output = widgets.Output()\n",
    "with recording_title_output:\n",
    "    print(\"Recording options:\")\n",
    "duration_input = widgets.FloatText(value=4, description='Duration:', disabled=False)\n",
    "offset_input = widgets.FloatText(value=1, description='Offset:', disabled=False)\n",
    "button_record = widgets.Button(value=False, description='Start', button_style='', icon='check')\n",
    "saving_file_name = widgets.Text(value=\"example.wav\", description='Save file:', disabled=False)\n",
    "\n",
    "## Files menu\n",
    "file_title_output = widgets.Output()\n",
    "with file_title_output:\n",
    "    print(\"File options:\")\n",
    "folder_input = widgets.Text(value=\"../audios/temp/\", description='Folder:', disabled=False, layout={\"width\":\"200px\",\"!padding-left\":\"0px\"})\n",
    "files_in_folder = get_folder_files(folder_input.value) \n",
    "files_dropdown = widgets.Dropdown(options= files_in_folder ,value=files_in_folder[0],description='',disabled=False, layout = {\"width\":\"200px\", \"text-align\":\"center\"})\n",
    "\n",
    "button_play = widgets.Button(value=False, description='Play', button_style='', icon='check', layout = {\"width\":\"50%\"})\n",
    "button_plot = widgets.Button(value=False, description='Plot', button_style='', icon='check', layout = {\"width\":\"50%\"})\n",
    "\n",
    "## Preprocessing menu\n",
    "processing_output = widgets.Output()\n",
    "with processing_output:\n",
    "    print(\"Processing options:\")\n",
    "noisy_audio_input = widgets.Text(value=\"noise.wav\", description='Noise:', disabled=False)\n",
    "reduce_noise_button = widgets.Button(value=False, description='Reduce noise', button_style='', icon='check')\n",
    "\n",
    "ngrad_freq_input = widgets.IntText(value=2, description='ngrad_freq:', disabled=False, layout = {\"width\":\"150px\"})\n",
    "ngrad_time_input = widgets.IntText(value=4, description='ngrad_time:', disabled=False, layout = {\"width\":\"150px\"})\n",
    "\n",
    "fft_length_input = widgets.IntText(value=2048, description='fft_length:', disabled=False, layout = {\"width\":\"150px\"})\n",
    "hop_length_input = widgets.IntText(value=512, description='hop_length:', disabled=False, layout = {\"width\":\"150px\"})\n",
    "\n",
    "n_std_thresh_input = widgets.FloatText(value=1.0, description='n_std_thresh:', disabled=False,layout = {\"width\":\"150px\"})\n",
    "prop_decrease_input = widgets.FloatText(value=0.8, description='prop_decrease:', disabled=False,layout = {\"width\":\"150px\"})\n",
    "\n",
    "ngrad_filter_box = widgets.HBox([ngrad_freq_input, ngrad_time_input])\n",
    "windows_length_box =  widgets.HBox([fft_length_input, hop_length_input])\n",
    "threshold_prop_box = widgets.HBox([n_std_thresh_input,prop_decrease_input ])\n",
    "# Create main Box \n",
    "play_plot_buttons_box = widgets.HBox([button_play, button_plot])\n",
    "recording_box = widgets.VBox([recording_title_output,saving_file_name,duration_input,offset_input, button_record], layout={'border': '1px solid black'})\n",
    "folder_box = widgets.VBox([file_title_output, folder_input, files_dropdown, play_plot_buttons_box],  layout={'border': '1px solid black'})\n",
    "processing_box = widgets.VBox([processing_output,ngrad_filter_box,windows_length_box,threshold_prop_box, noisy_audio_input,reduce_noise_button],  layout={'border': '1px solid black'})\n",
    "\n",
    "panel_box = widgets.VBox([folder_box, recording_box,processing_box])\n",
    "recorder_box = widgets.HBox([panel_box, figure_box])\n",
    "\n",
    "# Callback functions\n",
    "def selected_filename():\n",
    "    return folder_input.value + files_dropdown.value\n",
    "\n",
    "def selected_noisy_filename():\n",
    "    return folder_input.value + files_dropdown.value\n",
    "\n",
    "def reduce_noise_callback(button):\n",
    "    rate, audio_data = wavfile.read( selected_filename())\n",
    "    rate, noisy_data = wavfile.read(selected_noisy_filename())\n",
    "    audio_data = audio_data.astype(float)\n",
    "    noisy_data = noisy_data.astype(float)\n",
    "\n",
    "    reduced_noise = nr.reduce_noise(audio_clip = audio_data, noise_clip = noisy_data,\n",
    "                                    n_grad_freq=ngrad_freq_input.value, n_grad_time=ngrad_time_input.value,\n",
    "                                    n_fft=fft_length_input.value, win_length=fft_length_input.value, hop_length=hop_length_input.value,\n",
    "                                    n_std_thresh=n_std_thresh_input.value, prop_decrease= prop_decrease_input.value,\n",
    "                                    pad_clipping=True, verbose = False)\n",
    "    \n",
    "    reduced_noise = reduced_noise.astype(np.int16)\n",
    "    \n",
    "    reduced_noise_filename =  selected_filename().split(\".wav\")[0] + \"_rn.wav\"\n",
    "    wavfile.write(reduced_noise_filename, audio_params.sample_rate, reduced_noise)\n",
    "    \n",
    "    files_in_folder = get_folder_files(folder_input.value) \n",
    "    files_dropdown.options= files_in_folder\n",
    "    files_dropdown.value= reduced_noise_filename.split(\"/\")[-1]\n",
    "        \n",
    "def play_button_callback(button):\n",
    "    play_audio(audio_params, selected_filename())\n",
    "\n",
    "def plot_file_callback(button):\n",
    "    figure_box.children = [get_audio_menu_wav_file( selected_filename())]\n",
    "\n",
    "def folder_input_submit_callback(folder_input):\n",
    "    files_in_folder = get_folder_files(folder_input.value) \n",
    "    files_dropdown.options= files_in_folder\n",
    "    if len(files_in_folder)>0:\n",
    "        files_dropdown.value=files_in_folder[0]\n",
    "    else:\n",
    "        files_dropdown.value = None\n",
    "    \n",
    "def record_button_callback(button):\n",
    "    if button.description == \"Start\":\n",
    "        button.description = \"Recording\"\n",
    "        with mic as source:\n",
    "            audio = r.record(source,duration = duration_input.value, offset = offset_input.value)\n",
    "            \n",
    "        with open(folder_input.value + saving_file_name.value, \"wb\") as f:\n",
    "            f.write(audio.get_wav_data())\n",
    "        \n",
    "        button.description = \"Start\"\n",
    "        files_in_folder = get_folder_files(folder_input.value) \n",
    "        files_dropdown.options = files_in_folder\n",
    "        files_dropdown.value= saving_file_name.value\n",
    "    \n",
    "\n",
    "# Assign callback functions\n",
    "button_record.on_click(record_button_callback)\n",
    "button_play.on_click(play_button_callback)\n",
    "button_plot.on_click(plot_file_callback)\n",
    "folder_input.on_submit(folder_input_submit_callback)\n",
    "reduce_noise_button.on_click(reduce_noise_callback)\n",
    "\n",
    "# Display recorder.\n",
    "plot_file_callback(None)\n",
    "display(recorder_box)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Listen in background\n",
    "\n",
    "Create a thread that records in the background and puts the sentences read into queue that has as input the "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Tuning the parameters of the listening."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing data\n",
    "\n",
    "We initialize a queue where the recorded sentences will be added from the recordings taken in another thread.\n",
    "We give that queue to a closure that will return a function that stores the recordings in such queue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "recordings_queue = Queue()\n",
    "\n",
    "put_data_in_queue_callback = put_data_in_queue_closure(recordings_queue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We start listening\n",
    "\n",
    "The listen in thread function will call the listen() function in another thread and then apply the put_audio_data_in_queue_callback() to every sentence. So the sentences will be stored in the queue, to be processed later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop listening will stop the thread\n",
    "stop_listening = listen_in_a_thread(r, mic, put_data_in_queue_callback, phrase_time_limit = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consume the audios put in the queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcribing...: as the other was\n",
      "Transcribing...: "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-e512934152b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mi\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mtranscribe_queue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecordings_queue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranscriber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-28-e512934152b6>\u001b[0m in \u001b[0;36mtranscribe_queue\u001b[0;34m(q, transcriber, folder_recordings)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Transcribing...: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtranscribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename_audio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sentence\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/VScode/pyramidman/pyramidman/deepspeech_tools.py\u001b[0m in \u001b[0;36mtranscribe\u001b[0;34m(args, filepath, verbose)\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Running inference.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0minference_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m     \u001b[0maudio_metadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msttWithMetadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0minference_end\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0minference_start\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python36/lib/python3.7/site-packages/deepspeech/__init__.py\u001b[0m in \u001b[0;36msttWithMetadata\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mMetadata\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \"\"\"\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdeepspeech\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSpeechToTextWithMetadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_impl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcreateStream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "args = DeepSpeechArgs()\n",
    "transcriber = lambda x:  transcribe(args, x)\n",
    "\n",
    "def transcribe_queue(q, transcriber, folder_recordings = '../audios/temp/'):\n",
    "    while(True):\n",
    "        i = 0\n",
    "        audio = q.get()\n",
    "        filename_audio = f'{folder_recordings}{i}.wav'\n",
    "        with open(filename_audio, \"wb\") as f:\n",
    "            f.write(audio.get_wav_data())\n",
    "\n",
    "        print(\"Transcribing...: \", end=\"\")\n",
    "        metadata = transcribe(args, filename_audio)\n",
    "        sentence = metadata[\"sentence\"]\n",
    "        print(sentence)\n",
    "        i+=1 \n",
    "\n",
    "transcribe_queue(recordings_queue, transcriber)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop listening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling this function requests that the background listener stop listening\n",
    "stop_listening(wait_for_stop=False)\n",
    "recordings_queue.empty()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
