{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recording Studio\n",
    "\n",
    "This notebook is intended to record, preprocess and save the audios that will be later used by pyramidman assistant. It will make use of the speech recognizing as well for practical purposes, but theses will not be explained in this notebook, but rather in the third one.\n",
    "\n",
    "This notebook focuses on making a proper listener in another thread that writes the audio data into a Queue that is later consumed by the main thread and in having a simple Recording Studio for making the audios for pyramidman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/montoya/anaconda3/envs/python36/lib/python3.7/site-packages/tqdm/autonotebook.py:17: TqdmExperimentalWarning:\n",
      "\n",
      "Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pyramidman.audio_parameters import AudioParameters\n",
    "from pyramidman.basic_audio_IO import play_audio, record_audio\n",
    "from pyramidman.audio_utils import get_available_microphones, get_sysdefault_microphone_index, get_all_devices_str\n",
    "from pyramidman.queue_utils import record_with_queue\n",
    "from pyramidman.unwrapper import unwrap\n",
    "from pyramidman.speech_recognizing import recognize_speech_from_mic\n",
    "from pyramidman.hieroglyph import plot_timeseries_range_slider, create_tabs, plot_spectrogram\n",
    "from pyramidman.hieroglyph import add_word_annotations\n",
    "\n",
    "from pyramidman.Ihy import get_audio_menu_wav_file\n",
    "from pyramidman.signal_processing import get_spectrogram\n",
    "\n",
    "from pyramidman.queue_utils import put_audio_data_in_queue_callback_closure, listen_in_a_thread\n",
    "from pyramidman.audio_utils import calibrate_microphone\n",
    "\n",
    "import speech_recognition as sr\n",
    "from pyramidman.deepspeech_tools import transcribe, DeepSpeechArgs\n",
    "\n",
    "import plotly\n",
    "import time\n",
    "\n",
    "%matplotlib qt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import plotly.graph_objs as go\n",
    "from IPython.display import display\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "from scipy.io import wavfile\n",
    "import ipywidgets as widgets\n",
    "\n",
    "from queue import Queue\n",
    "import noisereduce as nr\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instantiate and calibrate microphone\n",
    "\n",
    "Ideally, we would like a background process in a thread that whenever a sentence is finished, it is translated and plotted. This is the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_params = AudioParameters()\n",
    "audio_params.set_sysdefault_microphone_index()\n",
    "audio_params.set_default_input_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_params.input_device_index = 6\n",
    "audio_params.sample_rate = 48000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calibrating microphone for 3 seconds.\n",
      "Calibrated\n"
     ]
    }
   ],
   "source": [
    "mic = audio_params.get_microphone()\n",
    "r = sr.Recognizer()\n",
    "\n",
    "calibrate_microphone(mic, r, duration = 3, dynamic_energy_threshold= False )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimize noise expetiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording\n",
      "Finished recording\n"
     ]
    }
   ],
   "source": [
    "file_to_record = \"../audios/temp/recording.wav\"\n",
    "record_audio(audio_params, seconds = 5, filename = file_to_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_audio(audio_params, file_to_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load data\n",
    "rate, data = wavfile.read(file_to_record)\n",
    "data = data.astype(float)\n",
    "# select section of data that is noise\n",
    "initial_noise_duration = 1\n",
    "noisy_part = data[:int(audio_params.sample_rate/initial_noise_duration)]\n",
    "\n",
    "# perform noise reduction\n",
    "reduced_noise = nr.reduce_noise(audio_clip=data, noise_clip=noisy_part, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_noise = reduced_noise.astype(np.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_to_record_reduced = \"../audios/temp/recording_reduced.wav\"\n",
    "wavfile.write(file_to_record_reduced, audio_params.sample_rate, reduced_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "564ec49277cd4a3686bab741ae54199d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tab(children=(FigureWidget({\n",
       "    'data': [{'line': {'color': 'deepskyblue'},\n",
       "              'name': 'AAPL High'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tabs = get_audio_menu_wav_file(file_to_record)\n",
    "display(tabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d84ef4ef0a6e437b8b410270416b62ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tab(children=(FigureWidget({\n",
       "    'data': [{'line': {'color': 'deepskyblue'},\n",
       "              'name': 'AAPL High'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tabs = get_audio_menu_wav_file(file_to_record_reduced)\n",
    "display(tabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(239616,) (239616,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([     0, 239616])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load some audio\n",
    "# Trim the beginning and ending silence\n",
    "\n",
    "yt, index = librosa.effects.trim(reduced_noise.astype(float),top_db=20, ref=np.max, frame_length=512*4, hop_length=256*4)\n",
    "print(yt.shape, reduced_noise.shape)\n",
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_to_record_reduced_cut = \"../audios/temp/recording_reduced_cut.wav\"\n",
    "wavfile.write(file_to_record_reduced_cut, audio_params.sample_rate, yt.astype(np.int16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8c70c8d5fe84278abc50625ca5b6297",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tab(children=(FigureWidget({\n",
       "    'data': [{'line': {'color': 'deepskyblue'},\n",
       "              'name': 'AAPL High'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tabs = get_audio_menu_wav_file(file_to_record_reduced_cut)\n",
    "display(tabs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transcribe to know which approach is better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_audio(audio_params, file_to_record)\n",
    "play_audio(audio_params, file_to_record_reduced)\n",
    "play_audio(audio_params, file_to_record_reduced_cut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = DeepSpeechArgs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcribe(args, file_to_record)[\"sentence\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcribe(args, file_to_record_reduced)[\"sentence\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcribe(args, file_to_record_reduced_cut)[\"sentence\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Listen in background\n",
    "\n",
    "Create a thread that records in the background and puts the sentences read into queue that has as input the "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning the parameters of the listening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.non_speaking_duration = 0.2\n",
    "r.pause_threshold = 1.2\n",
    "r.energy_threshold = 2500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Stop listening will stop the thread\n",
    "q = Queue()\n",
    "put_audio_data_in_queue_callback = put_audio_data_in_queue_callback_closure(r, mic, q)\n",
    "stop_listening = listen_in_a_thread(r, mic, put_audio_data_in_queue_callback, phrase_time_limit = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consume the audios put in the queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving... Transcribing...: you know that threshold\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "while(True):\n",
    "    \n",
    "    filename_mic = f'../audios/temp/{i}.wav'\n",
    "    audio = q.get()\n",
    "    \n",
    "    print(\"Saving... \", end = \"\")\n",
    "    with open(filename_mic, \"wb\") as f:\n",
    "        f.write(audio.get_wav_data())\n",
    "\n",
    "    \"\"\"     \n",
    "    if i == 0:\n",
    "        tabs = get_audio_menu_wav_file(filename_mic)\n",
    "        display(tabs)\n",
    "    else:\n",
    "        tabs_next = get_audio_menu_wav_file(filename_mic)\n",
    "        tabs.children = tabs_next.children\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Transcribing...: \", end=\"\")\n",
    "    args = DeepSpeechArgs()\n",
    "    metadata = transcribe(args, filename_mic)\n",
    "\n",
    "    print(metadata[\"sentence\"])\n",
    "    # add_word_annotations(tabs.children[0],metadata[\"words\"])\n",
    "    \n",
    "    if \"pyramid man\" in metadata[\"sentence\"]:\n",
    "        print(\"Activated broh\")\n",
    "        play_audio(audio_params, filename_mic)\n",
    "    \n",
    "    i+=1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop listening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calling this function requests that the background listener stop listening\n",
    "stop_listening(wait_for_stop=False)\n",
    "q.empty()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recording studio.\n",
    "\n",
    "We have created a simple plotly UI to record and save the audios for the pyramidman assistant. This can be reused in the future for extension of capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mic = audio_params.get_microphone()\n",
    "r = sr.Recognizer()\n",
    "\n",
    "figure_box = widgets.Box([go.FigureWidget()])\n",
    "\n",
    "def record_button_callback(button):\n",
    "    if button.description == \"Start\":\n",
    "        filename_mic = '../audios/temp/hello_world.wav'\n",
    "\n",
    "        with mic as source:\n",
    "            # audio = r.record(source,duration = 2)\n",
    "            audio = r.listen(source)\n",
    "            \n",
    "        with open(filename_mic, \"wb\") as f:\n",
    "            f.write(audio.get_wav_data())\n",
    "        \n",
    "        figure_box.children = [get_audio_menu_wav_file(filename_mic)]\n",
    "        \n",
    "button_record = widgets.Button(\n",
    "    value=False,\n",
    "    description='Start',\n",
    "    button_style='', # 'success', 'info', 'warning', 'danger' or ''\n",
    "    icon='check'\n",
    ")\n",
    "\n",
    "button_record.on_click(record_button_callback)\n",
    "\n",
    "                      \n",
    "recorder_box = widgets.VBox([button_record, figure_box])\n",
    "display(recorder_box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
