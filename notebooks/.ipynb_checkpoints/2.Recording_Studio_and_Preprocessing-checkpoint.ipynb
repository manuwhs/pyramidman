{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recording Studio\n",
    "\n",
    "This notebook is intended to record, preprocess and save the audios that will be later used by pyramidman assistant. It will make use of the speech recognizing as well for practical purposes, but theses will not be explained in this notebook, but rather in the third one.\n",
    "\n",
    "This notebook focuses on making a proper listener in another thread that writes the audio data into a Queue that is later consumed by the main thread and in having a simple Recording Studio for making the audios for pyramidman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/montoya/anaconda3/envs/python36/lib/python3.7/site-packages/tqdm/autonotebook.py:17: TqdmExperimentalWarning:\n",
      "\n",
      "Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pyramidman.audio_parameters import AudioParameters\n",
    "from pyramidman.basic_audio_IO import play_audio, record_audio\n",
    "from pyramidman.audio_utils import get_available_microphones, get_sysdefault_microphone_index, get_all_devices_str\n",
    "from pyramidman.queue_utils import record_with_queue\n",
    "from pyramidman.unwrapper import unwrap\n",
    "from pyramidman.speech_recognizing import recognize_speech_from_mic\n",
    "from pyramidman.hieroglyph import plot_timeseries_range_slider, create_tabs, plot_spectrogram\n",
    "from pyramidman.hieroglyph import add_word_annotations\n",
    "\n",
    "from pyramidman.Ihy import get_audio_menu_wav_file\n",
    "from pyramidman.signal_processing import get_spectrogram\n",
    "\n",
    "from pyramidman.queue_utils import put_audio_data_in_queue_callback_closure, listen_in_a_thread\n",
    "from pyramidman.audio_utils import calibrate_microphone, sample_noise\n",
    "\n",
    "from pyramidman.utils import get_folder_files\n",
    "import speech_recognition as sr\n",
    "from pyramidman.deepspeech_tools import transcribe, DeepSpeechArgs\n",
    "\n",
    "import plotly\n",
    "import time\n",
    "\n",
    "%matplotlib qt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import plotly.graph_objs as go\n",
    "from IPython.display import display\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "from scipy.io import wavfile\n",
    "import ipywidgets as widgets\n",
    "\n",
    "from queue import Queue\n",
    "import noisereduce as nr\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Instantiate and calibrate microphone\n",
    "\n",
    "Ideally, we would like a background process in a thread that whenever a sentence is finished, it is translated and plotted. This is the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_params = AudioParameters()\n",
    "audio_params.set_sysdefault_microphone_index()\n",
    "audio_params.set_default_input_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_params.input_device_index = 6\n",
    "audio_params.sample_rate = 48000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calibrating microphone for 1 seconds.\n",
      "Calibrated energy threshold:  5902.09206677274\n"
     ]
    }
   ],
   "source": [
    "mic = audio_params.get_microphone()\n",
    "r = sr.Recognizer()\n",
    "\n",
    "calibrate_microphone(mic, r, duration = 1, warmup_duration = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Reduce noise and trim signal\n",
    "\n",
    "In this section we show some of the functions used in order to reduce the noise of a recorded signal. The goal is mainly didactic, later in the Recording studio, we can play more with these functions to find the optimal preprocessing of the signal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Record sample audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording\n",
      "Finished recording\n"
     ]
    }
   ],
   "source": [
    "recorded_filepath = \"../audios/temp/recording.wav\"\n",
    "record_audio(audio_params, seconds = 5, filename = recorded_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_audio(audio_params, recorded_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Reduce noise\n",
    "\n",
    "We can apply several filters and advanced techniques. In this case we mainly used an advanced library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "\n",
    "noise_data = sample_noise(audio_params,r, mic, duration = 2, warmup = 2)\n",
    "\n",
    "rate, data = wavfile.read(recorded_filepath)\n",
    "data = data.astype(float)\n",
    "# perform noise reduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform the noise reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_noise = nr.reduce_noise(audio_clip=data, noise_clip=noise_data, verbose=False)\n",
    "reduced_noise = reduced_noise.astype(np.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyramidman.noisereduce_optimized import reduce_noise_optimized\n",
    "from pyramidman.noisereduce_optimized import noise_STFT_and_statistics\n",
    "\n",
    "reduced_noise = reduce_noise_optimized(audio_clip=data, noise_clip=noise_data)\n",
    "reduced_noise = reduced_noise.astype(np.int16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_params_list = noise_STFT_and_statistics(noise_data)\n",
    "noise_stft, noise_stft_db, mean_freq_noise, std_freq_noise, noise_thresh = noise_params_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyramidman.noisereduce_optimized import reduce_noise_optimized_closure\n",
    "reduce_noise_optimized_closurized = reduce_noise_optimized_closure(noise_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-7717.3916, -7192.6875, -6779.0684, ...,  4765.0747,  3966.8596,\n",
       "       -1815.4637], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduce_noise_optimized_closurized(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_noise = reduce_noise_optimized(audio_clip=data, precomputed_noise_parameters=noise_params_list)\n",
    "reduced_noise = reduced_noise.astype(np.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "recorded_filepath_noise_reduced = \"../audios/temp/recording_reduced.wav\"\n",
    "wavfile.write(recorded_filepath_noise_reduced, audio_params.sample_rate, reduced_noise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Trim signal\n",
    "\n",
    "Mainly at the beggining and end of the signal in order to reduce the length of the signal given later to the transcriber. It is mainly based on detecting when the power of the signal is below a given threshold. More advanced approaches based on the power at given frequencies could be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_noise_reduced_and_trimmed, index = librosa.effects.trim(reduced_noise.astype(float),top_db=20, ref=np.max, frame_length=512*4, hop_length=256*4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "recorded_filepath_noise_reduced_and_trimmed = \"../audios/temp/recording_reduced_and_trimmed.wav\"\n",
    "wavfile.write(recorded_filepath_noise_reduced_and_trimmed, audio_params.sample_rate, audio_noise_reduced_and_trimmed.astype(np.int16))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We also just trim the original signal for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_trimmed, index = librosa.effects.trim(data.astype(float),top_db=20, ref=np.max, frame_length=512*4, hop_length=256*4)\n",
    "\n",
    "recorded_filepath_trimmed = \"../audios/temp/recording_trimmed.wav\"\n",
    "wavfile.write(recorded_filepath_trimmed, audio_params.sample_rate, audio_trimmed.astype(np.int16))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3034b66cb2a464e9b08ee9b5f43028a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tab(children=(Tab(children=(FigureWidget({\n",
       "    'data': [{'line': {'color': 'deepskyblue'},\n",
       "              'name…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tabs_1 = get_audio_menu_wav_file(recorded_filepath)\n",
    "tabs_2 = get_audio_menu_wav_file(recorded_filepath_noise_reduced)\n",
    "tabs_3 = get_audio_menu_wav_file(recorded_filepath_trimmed)\n",
    "tabs_4 = get_audio_menu_wav_file(recorded_filepath_noise_reduced_and_trimmed)\n",
    "\n",
    "tabs = create_tabs([tabs_1, tabs_2, tabs_3, tabs_4], [\"Original\", \"Filtered\",\"Trimmed\", \"Filtered and Trimmered\"])\n",
    "display(tabs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Transcribe to know which approach is better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<DeepSpeechArgs>\tobject has children:\n",
      "    <str>\tmodel:\t../models/deepspeech/deepspeech-0.6.0-mo\n",
      "    <str>\tlm:\t../models/deepspeech/deepspeech-0.6.0-mo\n",
      "    <str>\ttrie:\t../models/deepspeech/deepspeech-0.6.0-mo\n",
      "    <int>\tbeam_width:\t500\n",
      "    <float>\tlm_alpha:\t0.75\n",
      "    <float>\tlm_beta:\t1.85\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "args = DeepSpeechArgs()\n",
    "unwrap(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'and it is also my treat documents attached to it with the savory and '"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "play_audio(audio_params, recorded_filepath)\n",
    "transcribe(args, recorded_filepath)[\"sentence\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'at a man attached to it and be sorry and '"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "play_audio(audio_params, recorded_filepath_noise_reduced)\n",
    "transcribe(args, recorded_filepath_noise_reduced)[\"sentence\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'and it is also my treat documents attached to it with the savory and '"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "play_audio(audio_params, recorded_filepath_trimmed)\n",
    "transcribe(args, recorded_filepath_trimmed)[\"sentence\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'at a man attached to it and be sorry and '"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "play_audio(audio_params, recorded_filepath_noise_reduced_and_trimmed)\n",
    "transcribe(args, recorded_filepath_noise_reduced_and_trimmed)[\"sentence\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Recording studio.\n",
    "\n",
    "We have created a simple plotly UI to record and save the audios for the pyramidman assistant. This can be reused in the future for extension of capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a08521f07914b3cb187cee55100bc28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(VBox(children=(Output(), Text(value='../audios/temp/', description='Folder:', la…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Global variables\n",
    "mic = audio_params.get_microphone()\n",
    "r = sr.Recognizer()\n",
    "\n",
    "# Box with the recordings to show.\n",
    "figure_box = widgets.Box([go.FigureWidget()], layout = {\"width\":\"70%\", \"height\":\"600px\"})\n",
    "\n",
    "######### Panel widgets \n",
    "\n",
    "## Recording menu\n",
    "recording_title_output = widgets.Output()\n",
    "with recording_title_output:\n",
    "    print(\"Recording options:\")\n",
    "duration_input = widgets.FloatText(value=4, description='Duration:', disabled=False)\n",
    "offset_input = widgets.FloatText(value=1, description='Offset:', disabled=False)\n",
    "button_record = widgets.Button(value=False, description='Start', button_style='', icon='check')\n",
    "saving_file_name = widgets.Text(value=\"example.wav\", description='Save file:', disabled=False)\n",
    "\n",
    "## Files menu\n",
    "file_title_output = widgets.Output()\n",
    "with file_title_output:\n",
    "    print(\"File options:\")\n",
    "folder_input = widgets.Text(value=\"../audios/temp/\", description='Folder:', disabled=False, layout={\"width\":\"200px\",\"!padding-left\":\"0px\"})\n",
    "files_in_folder = get_folder_files(folder_input.value) \n",
    "files_dropdown = widgets.Dropdown(options= files_in_folder ,value=files_in_folder[0],description='',disabled=False, layout = {\"width\":\"200px\", \"text-align\":\"center\"})\n",
    "\n",
    "button_play = widgets.Button(value=False, description='Play', button_style='', icon='check', layout = {\"width\":\"50%\"})\n",
    "button_plot = widgets.Button(value=False, description='Plot', button_style='', icon='check', layout = {\"width\":\"50%\"})\n",
    "\n",
    "## Preprocessing menu\n",
    "processing_output = widgets.Output()\n",
    "with processing_output:\n",
    "    print(\"Processing options:\")\n",
    "noisy_audio_input = widgets.Text(value=\"noise.wav\", description='Noise:', disabled=False)\n",
    "reduce_noise_button = widgets.Button(value=False, description='Reduce noise', button_style='', icon='check')\n",
    "\n",
    "ngrad_freq_input = widgets.IntText(value=2, description='ngrad_freq:', disabled=False, layout = {\"width\":\"150px\"})\n",
    "ngrad_time_input = widgets.IntText(value=4, description='ngrad_time:', disabled=False, layout = {\"width\":\"150px\"})\n",
    "\n",
    "fft_length_input = widgets.IntText(value=2048, description='fft_length:', disabled=False, layout = {\"width\":\"150px\"})\n",
    "hop_length_input = widgets.IntText(value=512, description='hop_length:', disabled=False, layout = {\"width\":\"150px\"})\n",
    "\n",
    "n_std_thresh_input = widgets.FloatText(value=1.0, description='n_std_thresh:', disabled=False,layout = {\"width\":\"150px\"})\n",
    "prop_decrease_input = widgets.FloatText(value=0.8, description='prop_decrease:', disabled=False,layout = {\"width\":\"150px\"})\n",
    "\n",
    "ngrad_filter_box = widgets.HBox([ngrad_freq_input, ngrad_time_input])\n",
    "windows_length_box =  widgets.HBox([fft_length_input, hop_length_input])\n",
    "threshold_prop_box = widgets.HBox([n_std_thresh_input,prop_decrease_input ])\n",
    "# Create main Box \n",
    "play_plot_buttons_box = widgets.HBox([button_play, button_plot])\n",
    "recording_box = widgets.VBox([recording_title_output,saving_file_name,duration_input,offset_input, button_record], layout={'border': '1px solid black'})\n",
    "folder_box = widgets.VBox([file_title_output, folder_input, files_dropdown, play_plot_buttons_box],  layout={'border': '1px solid black'})\n",
    "processing_box = widgets.VBox([processing_output,ngrad_filter_box,windows_length_box,threshold_prop_box, noisy_audio_input,reduce_noise_button],  layout={'border': '1px solid black'})\n",
    "\n",
    "panel_box = widgets.VBox([folder_box, recording_box,processing_box])\n",
    "recorder_box = widgets.HBox([panel_box, figure_box])\n",
    "\n",
    "# Callback functions\n",
    "def selected_filename():\n",
    "    return folder_input.value + files_dropdown.value\n",
    "\n",
    "def selected_noisy_filename():\n",
    "    return folder_input.value + files_dropdown.value\n",
    "\n",
    "def reduce_noise_callback(button):\n",
    "    rate, audio_data = wavfile.read( selected_filename())\n",
    "    rate, noisy_data = wavfile.read(selected_noisy_filename())\n",
    "    audio_data = audio_data.astype(float)\n",
    "    noisy_data = noisy_data.astype(float)\n",
    "\n",
    "    reduced_noise = nr.reduce_noise(audio_clip = audio_data, noise_clip = noisy_data,\n",
    "                                    n_grad_freq=ngrad_freq_input.value, n_grad_time=ngrad_time_input.value,\n",
    "                                    n_fft=fft_length_input.value, win_length=fft_length_input.value, hop_length=hop_length_input.value,\n",
    "                                    n_std_thresh=n_std_thresh_input.value, prop_decrease= prop_decrease_input.value,\n",
    "                                    pad_clipping=True, verbose = False)\n",
    "    \n",
    "    reduced_noise = reduced_noise.astype(np.int16)\n",
    "    \n",
    "    reduced_noise_filename =  selected_filename().split(\".wav\")[0] + \"_rn.wav\"\n",
    "    wavfile.write(reduced_noise_filename, audio_params.sample_rate, reduced_noise)\n",
    "    \n",
    "    files_in_folder = get_folder_files(folder_input.value) \n",
    "    files_dropdown.options= files_in_folder\n",
    "    files_dropdown.value= reduced_noise_filename.split(\"/\")[-1]\n",
    "        \n",
    "def play_button_callback(button):\n",
    "    play_audio(audio_params, selected_filename())\n",
    "\n",
    "def plot_file_callback(button):\n",
    "    figure_box.children = [get_audio_menu_wav_file( selected_filename())]\n",
    "\n",
    "def folder_input_submit_callback(folder_input):\n",
    "    files_in_folder = get_folder_files(folder_input.value) \n",
    "    files_dropdown.options= files_in_folder\n",
    "    files_dropdown.value=files_in_folder[0]\n",
    "    \n",
    "def record_button_callback(button):\n",
    "    if button.description == \"Start\":\n",
    "        button.description = \"Recording\"\n",
    "        with mic as source:\n",
    "            audio = r.record(source,duration = duration_input.value, offset = offset_input.value)\n",
    "            \n",
    "        with open(folder_input.value + saving_file_name.value, \"wb\") as f:\n",
    "            f.write(audio.get_wav_data())\n",
    "        \n",
    "        button.description = \"Start\"\n",
    "        files_in_folder = get_folder_files(folder_input.value) \n",
    "        files_dropdown.options = files_in_folder\n",
    "        files_dropdown.value= saving_file_name.value\n",
    "    \n",
    "\n",
    "# Assign callback functions\n",
    "button_record.on_click(record_button_callback)\n",
    "button_play.on_click(play_button_callback)\n",
    "button_plot.on_click(plot_file_callback)\n",
    "folder_input.on_submit(folder_input_submit_callback)\n",
    "reduce_noise_button.on_click(reduce_noise_callback)\n",
    "\n",
    "# Display recorder.\n",
    "plot_file_callback(None)\n",
    "display(recorder_box)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Listen in background\n",
    "\n",
    "Create a thread that records in the background and puts the sentences read into queue that has as input the "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Tuning the parameters of the listening."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing data\n",
    "\n",
    "We initialize a queue where the recorded sentences will be added from the recordings taken in another thread.\n",
    "We give that queue to a closure that will return a function that stores the recordings in such queue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "transcribed_recoding_queue = Queue()\n",
    "\n",
    "put_audio_data_in_queue_callback = put_audio_data_in_queue_callback_closure(r, mic, transcribed_recoding_queue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We start listening\n",
    "\n",
    "The listen in thread function will call the listen() function in another thread and then apply the put_audio_data_in_queue_callback() to every sentence. So the sentences will be stored in the queue, to be processed later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop listening will stop the thread\n",
    "stop_listening = listen_in_a_thread(r, mic, put_audio_data_in_queue_callback, phrase_time_limit = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consume the audios put in the queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving... Transcribing...: dat de deer like you have me but a folder and and you have their devising the men i would call them and improvises in his life in a play ground\n",
      "Saving... Transcribing...: i did fate is real and the fat evil as it oceanlike all as a reason\n",
      "Saving... Transcribing...: some of it now he doesn't yes it's not humour\n",
      "Saving... Transcribing...: did you plaise it leading to a war now and try and secure but i\n",
      "Saving... Transcribing...: this legion libertine a weishaupt the polaris\n",
      "Saving... Transcribing...: a foot into about parentage or a dusty wind arising of what everything said this like this next is have i concentered\n",
      "Saving... Transcribing...: like as it on disaster many are you used to hold that a eetes and i summarized\n",
      "Saving... Transcribing...: but even if this is where probably the lid afternoon itinerancy in one the one as is a pretence the bedouin\n",
      "Saving... Transcribing...: i lie there better mode of there like a gay just went in to eat in a cover of hours i once in us reply be \n",
      "Saving... Transcribing...: in design as i atalantis but we avestan back so we did to a talkin about this you can of ascribing all at this the light they put out sometimes and we know\n",
      "Saving... Transcribing...: it is principle there the other were there within this life the numismatist i too much moment for the unaware of it as i think only \n",
      "Saving... Transcribing...: industry where we like but it's all distaste there on disliking this time by it and as the virago about this i can't see the outer room like the order\n",
      "Saving... Transcribing...: and is it not know what this is i i i i strike this side when will you be a orangemen raise no the sheet so\n",
      "Saving... Transcribing...: for a time it is it's the saddest i potation do that what i got up which is a a then give you like go\n",
      "Saving... Transcribing...: "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-98590b95708f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Transcribing...: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDeepSpeechArgs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtranscribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename_mic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sentence\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/VScode/pyramidman/pyramidman/deepspeech_tools.py\u001b[0m in \u001b[0;36mtranscribe\u001b[0;34m(args, filepath, verbose)\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Running inference.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0minference_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m     \u001b[0maudio_metadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msttWithMetadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0minference_end\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0minference_start\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python36/lib/python3.7/site-packages/deepspeech/__init__.py\u001b[0m in \u001b[0;36msttWithMetadata\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mMetadata\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \"\"\"\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdeepspeech\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSpeechToTextWithMetadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_impl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcreateStream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "\n",
    "args = DeepSpeechArgs()\n",
    "transcriber = lambda x:  transcribe(args, x)\n",
    "\n",
    "        \n",
    "def transcribe_queue(q, transcriber, folder_recordings = '../audios/temp/'):\n",
    "    \n",
    "    while(True):\n",
    "        audio = q.get()\n",
    "        filename_audio = f'{folder_recordings}{i}.wav'\n",
    "        with open(filename_mic, \"wb\") as f:\n",
    "            f.write(audio.get_wav_data())\n",
    "\n",
    "        print(\"Transcribing...: \", end=\"\")\n",
    "        metadata = transcribe(args, filename_mic)\n",
    "        sentence = metadata[\"sentence\"]\n",
    "        print(sentence)\n",
    "        i+=1 \n",
    "\n",
    "        \n",
    "if \"pyramid man\" in metadata[\"sentence\"]:\n",
    "    print(\"Activated broh\")\n",
    "    play_audio(audio_params, filename_mic)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop listening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calling this function requests that the background listener stop listening\n",
    "stop_listening(wait_for_stop=False)\n",
    "q.empty()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
