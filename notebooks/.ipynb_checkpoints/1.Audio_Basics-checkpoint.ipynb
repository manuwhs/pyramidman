{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio Basics\n",
    "\n",
    "A walkthough the different functions in pyramidman that handle the recording, playing and processing of audio. It is mainly for didactic and testing purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'thread_listen_callback' from 'pyramidman.queue_utils' (/home/montoya/Desktop/VScode/pyramidman/pyramidman/queue_utils.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-6c8c73116d81>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyramidman\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignal_processing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_spectrogram\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyramidman\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mthread_listen_callback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlisten_in_a_thread\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyramidman\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudio_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcalibrate_microhpone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'thread_listen_callback' from 'pyramidman.queue_utils' (/home/montoya/Desktop/VScode/pyramidman/pyramidman/queue_utils.py)"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pyramidman.audio_parameters import AudioParameters\n",
    "from pyramidman.basic_audio_IO import play_audio, record_audio\n",
    "from pyramidman.audio_utils import get_available_microphones, get_sysdefault_microphone_index, get_all_devices_str\n",
    "from pyramidman.queue_utils import record_with_queue\n",
    "from pyramidman.unwrapper import unwrap\n",
    "from pyramidman.speech_recognizing import recognize_speech_from_mic\n",
    "\n",
    "from pyramidman.hieroglyph import plot_timeseries_range_slider, create_tabs, plot_spectrogram\n",
    "from pyramidman.hieroglyph import add_word_annotations\n",
    "\n",
    "from pyramidman.Ihy import get_audio_menu_wav_file\n",
    "from pyramidman.signal_processing import get_spectrogram\n",
    "\n",
    "from pyramidman.queue_utils import thread_listen_callback, listen_in_a_thread\n",
    "from pyramidman.audio_utils import calibrate_microhpone\n",
    "\n",
    "from pyramidman.deepspeech_tools import transcribe, DeepSpeechArgs\n",
    "from pyramidman.deepspeech_tools import words_from_metadata\n",
    "\n",
    "import speech_recognition as sr\n",
    "import time\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "from scipy.io import wavfile\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib qt\n",
    "\n",
    "import plotly.graph_objs as go\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instance of the class AudioParameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<AudioParameters>\tobject has children:\n",
      "    <int>\tchunk:\t2048\n",
      "    <int>\tsample_format:\t8\n",
      "    <NoneType>\tsubtype:\tNone\n",
      "    <int>\tchannels:\t1\n",
      "    <int>\tsample_rate:\t48000\n",
      "    <int>\tinput_device_index:\t0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "audio_params = AudioParameters()\n",
    "unwrap(audio_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'HDA Intel PCH: ALC295 Analog (hw:0,0)',\n",
       " 'hostapi': 0,\n",
       " 'max_input_channels': 2,\n",
       " 'max_output_channels': 0,\n",
       " 'default_low_input_latency': 0.005804988662131519,\n",
       " 'default_low_output_latency': -1.0,\n",
       " 'default_high_input_latency': 0.034829931972789115,\n",
       " 'default_high_output_latency': -1.0,\n",
       " 'default_samplerate': 44100.0}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_params.get_input_device_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information of avilable devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0 HDA Intel PCH: ALC295 Analog (hw:0,0), ALSA (2 in, 0 out)\n",
       "  1 HDA Intel PCH: HDMI 0 (hw:0,3), ALSA (0 in, 8 out)\n",
       "  2 HDA Intel PCH: HDMI 1 (hw:0,7), ALSA (0 in, 8 out)\n",
       "  3 HDA Intel PCH: HDMI 2 (hw:0,8), ALSA (0 in, 8 out)\n",
       "  4 HDA Intel PCH: HDMI 3 (hw:0,9), ALSA (0 in, 8 out)\n",
       "  5 HDA Intel PCH: HDMI 4 (hw:0,10), ALSA (0 in, 8 out)\n",
       "  6 sysdefault, ALSA (128 in, 0 out)\n",
       "  7 hdmi, ALSA (0 in, 8 out)\n",
       "  8 pulse, ALSA (32 in, 32 out)\n",
       "* 9 default, ALSA (32 in, 32 out)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_all_devices_str()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': 'HDA Intel PCH: ALC295 Analog (hw:0,0)',\n",
       " '6': 'sysdefault',\n",
       " '8': 'pulse',\n",
       " '9': 'default'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "devices_dict = get_available_microphones()\n",
    "devices_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play audio\n",
    "\n",
    "We pass it a AudioParameters instance, from it, it is going to get chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_to_play = \"../audios/standard/english.wav\"\n",
    "play_audio(audio_params, filename = file_to_play )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recording audio\n",
    "\n",
    "First we should select a proper mycrophone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<AudioParameters>\tobject has children:\n",
      "    <int>\tchunk:\t2048\n",
      "    <int>\tsample_format:\t8\n",
      "    <NoneType>\tsubtype:\tNone\n",
      "    <int>\tchannels:\t1\n",
      "    <int>\tsample_rate:\t48000\n",
      "    <int>\tinput_device_index:\t6\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "audio_params.set_sysdefault_microphone_index()\n",
    "audio_params.set_default_input_parameters()\n",
    "unwrap(audio_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call the recording function with the correct parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording\n",
      "Finished recording\n"
     ]
    }
   ],
   "source": [
    "file_to_record = \"../audios/temp/recording.wav\"\n",
    "record_audio(audio_params, seconds = 3, filename = file_to_record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play the recorded audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_audio(audio_params,  filename = file_to_record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Record using queues\n",
    "\n",
    "It seems better, with way less in between cuts that are probably because there is less delay due to processing of the chucks. But still as time grows, this shit takes too much time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Recording finished: \n"
     ]
    }
   ],
   "source": [
    "filename_w = \"../audios/temp/caec2.wav\"\n",
    "record_with_queue(audio_params, filename_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_audio(audio_params, filename_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a Microphone\n",
    "\n",
    "The speech_recognition library has a Microphone class that is helpful at recording data. We can get one instance directly from the AudioParameters class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "mic = audio_params.get_microphone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mic.device_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['HDA Intel PCH: ALC295 Analog (hw:0,0)',\n",
       " 'HDA Intel PCH: HDMI 0 (hw:0,3)',\n",
       " 'HDA Intel PCH: HDMI 1 (hw:0,7)',\n",
       " 'HDA Intel PCH: HDMI 2 (hw:0,8)',\n",
       " 'HDA Intel PCH: HDMI 3 (hw:0,9)',\n",
       " 'HDA Intel PCH: HDMI 4 (hw:0,10)',\n",
       " 'sysdefault',\n",
       " 'hdmi',\n",
       " 'pulse',\n",
       " 'default']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mic.list_microphone_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this instance to together with the recognizer of the library in order to capture text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<speech_recognition.AudioData at 0x7ff0734b5750>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = sr.Recognizer()\n",
    "with mic as source:                # use the default microphone as the audio source\n",
    "    audio = r.record(source, duration = 3)                   # listen for the first phrase and extract it into audio data\n",
    "\n",
    "audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<AudioData>\tobject has children:\n",
      "    <bytes>\tframe_data\n",
      "    <int>\tsample_rate:\t48000\n",
      "    <int>\tsample_width:\t2\n",
      "\n",
      "  <bytes>\tframe_data has children:\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unwrap(audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_mic = '../audios/temp/hello_world.wav'\n",
    "\n",
    "with mic as source:\n",
    "    audio = r.record(source,duration = 5)\n",
    "\n",
    "with open(filename_mic, \"wb\") as f:\n",
    "    f.write(audio.get_wav_data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_audio(audio_params, filename_mic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert audio to numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/montoya/anaconda3/envs/python36/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning:\n",
      "\n",
      "Numeric-style type codes are deprecated and will result in an error in the future.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "audio_array = np.frombuffer(audio.frame_data, \"Int16\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speech recognition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = recognize_speech_from_mic(audio_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Success : True\n",
      "Error   : Unable to recognize speech\n",
      "\n",
      "Text from Speech\n",
      "-----------------\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print('\\nSuccess : {}\\nError   : {}\\n\\nText from Speech\\n{}\\n\\n{}' \\\n",
    "      .format(response['success'],\n",
    "              response['error'],\n",
    "              '-'*17,\n",
    "              response['transcription']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting of the audiowave."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ec6465b4e9a412a8349e219476f5560",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tab(children=(FigureWidget({\n",
       "    'data': [{'line': {'color': 'deepskyblue'},\n",
       "              'name': 'AAPL High'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop listening\n"
     ]
    }
   ],
   "source": [
    "tabs = get_audio_menu_wav_file(filename_mic)\n",
    "display(tabs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Listen in background\n",
    "\n",
    "Ideally, we would like a background process in a thread that whenever a sentence is finished, it is translated and plotted. This is the following code.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please wait. Calibrating microphone...\n"
     ]
    }
   ],
   "source": [
    "mic = audio_params.get_microphone()\n",
    "r = sr.Recognizer()\n",
    "\n",
    "calibrate_microhpone(mic, r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listening\n",
      "Waiting Timeout Error\n",
      "Listening\n",
      "Stop listening\n",
      "Decoding\n",
      "Google Speech Recognition thinks you said:  hey how're you doing\n",
      "Listening\n",
      "Waiting Timeout Error\n",
      "Listening\n",
      "Stop listening\n",
      "Decoding\n",
      "Google Speech Recognition thinks you said:  where the thigh\n",
      "Listening\n",
      "Stop listening\n",
      "Decoding\n",
      "Google Speech Recognition thinks you said:  \n",
      "Listening\n",
      "Stop listening\n",
      "Decoding\n",
      "Google Speech Recognition thinks you said:  the fate of\n",
      "Listening\n",
      "Waiting Timeout Error\n",
      "Listening\n",
      "Stop listening\n",
      "Decoding\n",
      "Google Speech Recognition thinks you said:  why\n",
      "Listening\n",
      "Stop listening\n",
      "Decoding\n"
     ]
    }
   ],
   "source": [
    "# Stop listening will stop the thread\n",
    "stop_listening = listen_in_a_thread(r, mic, thread_listen_callback, phrase_time_limit = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google Speech Recognition thinks you said:  a it\n"
     ]
    }
   ],
   "source": [
    "# calling this function requests that the background listener stop listening\n",
    "stop_listening(wait_for_stop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model from file ../deepspeech-0.6.0-models/output_graph.pbmm\n",
      "Loaded model in 0.00935s.\n",
      "Loading language model from files ../deepspeech-0.6.0-models/lm.binary ../deepspeech-0.6.0-models/trie\n",
      "Loaded language model in 0.000254s.\n",
      "Warning: original sample rate (48000) is different than 16000hz. Resampling might produce erratic speech recognition.\n",
      "Running inference.\n",
      "Inference took 2.536s for 14.976s audio file.\n"
     ]
    }
   ],
   "source": [
    "args = DeepSpeechArgs()\n",
    "caca = transcribe(args, filename_mic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'words': [{'word': 'he', 'start_time ': 0.0, 'duration': 0.68}],\n",
       " 'confidence': -13.17073924950386}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'add_word_annotations' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-67131dc9e13f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtabs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_audio_menu_wav_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename_mic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0madd_word_annotations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtabs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcaca\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"words\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtabs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'add_word_annotations' is not defined"
     ]
    }
   ],
   "source": [
    "tabs = get_audio_menu_wav_file(filename_mic)\n",
    "add_word_annotations(tabs.children[0],caca[\"words\"])\n",
    "display(tabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2**15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
